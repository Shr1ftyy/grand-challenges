{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install snscrape\n%pip install h5py","metadata":{"execution":{"iopub.status.busy":"2022-09-03T00:41:49.865114Z","iopub.execute_input":"2022-09-03T00:41:49.866591Z","iopub.status.idle":"2022-09-03T00:42:18.732516Z","shell.execute_reply.started":"2022-09-03T00:41:49.866468Z","shell.execute_reply":"2022-09-03T00:42:18.730680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import snscrape.modules.twitter as sntwitter\nimport datetime as dt\nfrom textblob import TextBlob\nimport json\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import GRU,Dense,Embedding\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\nimport re\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-09-03T00:42:18.736368Z","iopub.execute_input":"2022-09-03T00:42:18.738034Z","iopub.status.idle":"2022-09-03T00:42:26.450797Z","shell.execute_reply.started":"2022-09-03T00:42:18.737969Z","shell.execute_reply":"2022-09-03T00:42:26.449319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MEAN_LEN_SENTENCES = 120","metadata":{"execution":{"iopub.status.busy":"2022-09-03T00:42:26.453135Z","iopub.execute_input":"2022-09-03T00:42:26.454120Z","iopub.status.idle":"2022-09-03T00:42:26.466606Z","shell.execute_reply.started":"2022-09-03T00:42:26.454073Z","shell.execute_reply":"2022-09-03T00:42:26.463425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizerJson = open(\"../input/financialsentimentmodel/tokenizer.json\", \"r\")\ntokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizerJson.read())\ntokenizerJson.close()\nprint(list(tokenizer.index_word.values())[:15])\n# Tokenized text example","metadata":{"execution":{"iopub.status.busy":"2022-09-03T00:42:26.469173Z","iopub.execute_input":"2022-09-03T00:42:26.469817Z","iopub.status.idle":"2022-09-03T00:42:27.173012Z","shell.execute_reply.started":"2022-09-03T00:42:26.469773Z","shell.execute_reply":"2022-09-03T00:42:27.171525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pattern = \"[^a-zA-Z0-9$,.]\"\nclean = [re.sub(pattern,\" \",\"$ESI on lows, down $1.50 to $2.50 BK a real possibility\").lower().strip()]\nt = tokenizer.texts_to_sequences(clean)\np = pad_sequences(t,maxlen=int(MEAN_LEN_SENTENCES))\nprint(p)\nmodel = tf.keras.models.load_model(\"../input/financialsentimentmodel/modelWeights\")\nprint(model.summary())\nmodel.predict(p)","metadata":{"execution":{"iopub.status.busy":"2022-09-03T00:42:27.177094Z","iopub.execute_input":"2022-09-03T00:42:27.177529Z","iopub.status.idle":"2022-09-03T00:42:42.200091Z","shell.execute_reply.started":"2022-09-03T00:42:27.177497Z","shell.execute_reply":"2022-09-03T00:42:42.198701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sentiment polarity: the difference between the number of positive and negative tweets as\na fraction of non-neutral tweets","metadata":{}},{"cell_type":"code","source":"ticks = [\"AAPL\", \"AXP\", \"BA\", \"CAT\", \"CSCO\", \"CVX\", \"NKE\", \"GS\", \"IBM\", \"INTC\", \"KO\", \"MMM\", \"MSFT\", \"V\", \"XOM\"]\nfor T in ticks:  \n  tweets = {}\n  labels = ['negative', 'neutral', 'positive']\n  numdays = 294\n  startDate = dt.datetime(2019, 2, 28)\n  startDate = dt.datetime(2019, 2, 28)\n  rawDates = [(startDate + dt.timedelta(days=x)) for x in range(numdays)]\n  date_list = [d.strftime(\"%Y-%m-%d\") for d in rawDates]\n  scraped = 0\n  print(\"starting\")\n  \n  for d in range(1, numdays):\n  #   i = 0\n    polarSum = 0\n    currentDate = date_list[d-1]\n    s = 0\n    freq = {'positive':0, 'negative':0, 'neutral':0}\n    if currentDate not in tweets.keys():\n      tweets[currentDate] = {'Date': currentDate, 'tweets': [], 'sentiment': 0, 'mode_polar':0, 'neutral': 0, 'volume': 0, 'freq': {}}\n    # dailyTweets = []\n  #   for i, tweet in enumerate(sntwitter.TwitterSearchScraper(f\"(feeling) OR (i am) OR (i'm) OR (i feel) OR (stock) OR (market) OR (economy) OR (trading) since:{date_list[d-1]} until:{date_list[d]} -filter:links -filter:replies\").get_items()):\n  #   if \n    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(f\"${T} since:{date_list[d-1]} until:{date_list[d]} -filter:links -filter:replies\").get_items()):\n      if i >= 5000:\n        break;\n  \n      pattern = \"[^a-zA-Z0-9$,.]\"\n      cleaned = [re.sub(pattern,\" \",tweet.content).lower().strip()]\n      tokenized = tokenizer.texts_to_sequences(cleaned)\n      padded = pad_sequences(tokenized,maxlen=int(MEAN_LEN_SENTENCES))\n      out = model.predict(padded)[0].tolist()\n      sentiment = labels[out.index(max(out))]\n      freq[sentiment] += 1\n  \n      entry = {\n        'content': tweet.content, \n        'date': currentDate, \n        'user': tweet.username,\n        'polarity': sentiment,\n        'neuralOuts': list(out)  \n      }\n  \n      tweets[currentDate]['tweets'].append(entry)\n      s += 1\n  \n    lenScraped = len(tweets[currentDate]['tweets']) \n    if lenScraped > 0:\n      tweets[currentDate]['mode_polar'] = list(freq.keys())[list(freq.values()).index(max(list(freq.values())))]\n    \n      scraped += lenScraped\n      tweets[currentDate]['volume'] = lenScraped\n      tweets[currentDate]['freq'] = freq\n      if freq['positive'] + freq['negative'] > 0:\n        tweets[currentDate]['sentiment'] = (freq['positive'] - freq['negative'])/(freq['positive'] + freq['negative'])\n      else:\n        tweets[currentDate]['sentiment'] = 0\n    print(f\"Ticker: {T}\")    \n    print(lenScraped)\n    print(len(tweets.keys()))\n    print(tweets.keys())\n  # f = open('tweets.csv', 'w+')\n  \n  jsonData = json.dumps(tweets)\n  f = open(f'tweet{T}RNN.json', 'w+')\n  f.write(jsonData)\n  f.close()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-09-03T00:42:42.202100Z","iopub.execute_input":"2022-09-03T00:42:42.202452Z"},"trusted":true},"execution_count":null,"outputs":[]}]}